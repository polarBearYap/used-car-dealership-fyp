{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9edccc9",
   "metadata": {},
   "source": [
    "**Author**: Yap Jheng Khin\n",
    "\n",
    "**FYP II Title**: Used car dealership web application\n",
    "\n",
    "**Purpose**:\n",
    "1. This notebook explains:\n",
    "    - The inputs expected by Tree SHAP explainer to support adaptive random forest regressor.\n",
    "    - The validation of tree weights extraction process for adaptive random forest regressor.\n",
    "    - The inputs expected by Tree SHAP explainer to support adaptive random forest classifier.\n",
    "    - The validation of tree weights extraction process for adaptive random forest classifier.\n",
    "2. Input: \n",
    "    - Fitted car price model (Scikit-learn random forest regressor).\n",
    "    - Fitted lead scoring model (Scikit-learn random forest classifier).\n",
    "    - Dictionary containing the extracted weights for the car price model.\n",
    "    - Dictionary containing the extracted weights for the lead scoring model.\n",
    "\n",
    "**Execution time**: At most 10 minute in Jupyter Notebook.\n",
    "\n",
    "**Important**: This notebook is part of the section in *FYP2_ARF_to_Dict_Conversion.ipynb*. The section is separated into a new file since it must be run on a different Python interpreter. It is because, as of April 2021, SHAP library and River library has conflicting dependenices on Numpy library. SHAP library requires Numpy version of 1.21.5 while River library requires Numpy version of 1.22.3. The import of both libraries will fail if the respective requirements are not met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c880e509",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b41098c",
   "metadata": {},
   "source": [
    "Ensure that the current Python interpreter path is correct. For example, if the **SHAP conda environment** is named as **arf_conda_exp_env**, the expected `sys.executable` should be C:\\Users\\User\\miniconda3\\envs\\\\**arf_conda_exp_env**\\\\python.exe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "156c86ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\miniconda3\\envs\\arf_conda_exp_env\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77cc2924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SHAP library\n",
    "import shap\n",
    "\n",
    "# Scikit-learn library\n",
    "from sklearn import tree as sklearn_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "# User-defined libraries\n",
    "from general_utils import deserialize_arf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b25cc4",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd3da8f",
   "metadata": {},
   "source": [
    "For initializing Tree SHAP explainer, the function internally extract the tree weights from an API-specific tree models before \n",
    "storing the information in the `TreeEnsemble` class instance. The function directly supports the weights extraction of tree models from commonly used API like Scikit-learn and XGBoost. However, the function does not directly supports the weight extraction of `AdaptiveRandomForestRegressor` from River API. Instead, the function accepts a Python dictionary that stores the tree weights which are `children_left`, `children_right`, `features`, `thresholds`, `node_sample_weight`, and `values`. Thus, the tree weights must be manually extracted into a dictionary.\n",
    "\n",
    "To ensure the adaptive random forest regressor's tree weights are passed in the correct format, the author initializes a Tree SHAP explainer object with the random forest regressor implemented by Scikit-learn. The tree weights are then retrieved from `TreeEnsemble` class instance to understand and prove the correctness of the dictionary values that are passed into the explainer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32821eff",
   "metadata": {},
   "source": [
    "## Tree Weights Extraction (Scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5cb96",
   "metadata": {},
   "source": [
    "Load  the fitted Scikit-learn random forest regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4c3db15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputs/car_price/trf_rg.pkl', 'rb') as f:\n",
    "    cp_trf_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a99402",
   "metadata": {},
   "source": [
    "Initialize the SHAP tree explainer with the random forest regressor implemented by Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3287627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_tree_exp = shap.TreeExplainer(model = cp_trf_model, \n",
    "                                 feature_perturbation = 'tree_path_dependent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c117ec96",
   "metadata": {},
   "source": [
    "Based on the <a href=\"https://github.com/slundberg/shap/blob/master/shap/explainers/_tree.py#L613\">SHAP's source code</a>, the expected dictionary structure is shown below:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'internal_dtype': <?>,\n",
    "    'input_dtype'   : <?>,\n",
    "    'objective'     : <?>,\n",
    "    'tree_output'   : <?>,\n",
    "    'base_offset'   : <?>,\n",
    "    'trees'         : <?>\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e656a",
   "metadata": {},
   "source": [
    "The lookup table is retrieved from the <a href=\"https://github.com/slundberg/shap/blob/master/shap/explainers/_tree.py#L585\">source code</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fe16f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_name_map = {\n",
    "    \"mse\": \"squared_error\",\n",
    "    \"variance\": \"squared_error\",\n",
    "    \"friedman_mse\": \"squared_error\",\n",
    "    \"reg:linear\": \"squared_error\",\n",
    "    \"reg:squarederror\": \"squared_error\",\n",
    "    \"regression\": \"squared_error\",\n",
    "    \"regression_l2\": \"squared_error\",\n",
    "    \"mae\": \"absolute_error\",\n",
    "    \"gini\": \"binary_crossentropy\",\n",
    "    \"entropy\": \"binary_crossentropy\",\n",
    "    \"reg:logistic\": \"binary_crossentropy\",\n",
    "    \"binary:logistic\": \"binary_crossentropy\",\n",
    "    \"binary_logloss\": \"binary_crossentropy\",\n",
    "    \"binary\": \"binary_crossentropy\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae0149",
   "metadata": {},
   "source": [
    "Below are the expected values for `internal_dtype`, `input_dtype`, `objective`, `tree_output`, and `base_offset` field. \n",
    "\n",
    "1. The `internal_dtype` defines the data type of the tree weights.\n",
    "\n",
    "2. The default valule of `input_dtype` is `np.float64`. If the model is implemented by Scikit-learn, the `input_dtype` value is changed to `np.float32` to get the exact matches to predictions, as documented in the <a href=\"https://github.com/slundberg/shap/blob/master/shap/explainers/_tree.py#L576\">source code</a>.\n",
    "\n",
    "3. The `objective` value defines which metric to explain when explaining the loss of the model. If the tree model is a classifier, then the explainer will explain binary crossentropy loss. Else if the tree model is a regressor, then the explainer will explain mean squared error loss. The `objective` value should be `squared_error` instead of `None`.\n",
    "\n",
    "4. For Scikit-learn regressor, the tree output is set to `raw_value` since the regressor directly output the target variable when `predict` is called. \n",
    "\n",
    "5. The default value of `base offset` is 0 unless the regressor applies gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4214b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class name that stores tree weights: <class 'shap.explainers._tree.TreeEnsemble'>\n",
      "internal_dtype : <class 'numpy.float64'>\n",
      "input_dtype    : <class 'numpy.float32'>\n",
      "objective      : None\n",
      "tree_output    : raw_value\n",
      "base_offset    : [0.]\n"
     ]
    }
   ],
   "source": [
    "cp_tree_ensemble = cp_tree_exp.model\n",
    "\n",
    "print(f'Class name that stores tree weights: {type(cp_tree_ensemble)}')\n",
    "print(f'internal_dtype : {cp_tree_ensemble.internal_dtype}')\n",
    "print(f'input_dtype    : {cp_tree_ensemble.input_dtype}')\n",
    "print(f'objective      : {cp_tree_ensemble.objective}')\n",
    "print(f'tree_output    : {cp_tree_ensemble.tree_output}')\n",
    "print(f'base_offset    : {cp_tree_ensemble.base_offset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d806952b",
   "metadata": {},
   "source": [
    "Below is the code that extracts the `internal_dtype`, and `objective` from the Scikit-learn object, as implemented in the <a href=\"https://github.com/slundberg/shap/blob/master/shap/explainers/_tree.py#L630\">SHAP's source code</a>. The remaining field values are fixed and pre-determined based on the API of the model.\n",
    "\n",
    "**Important**: Note that there is a bug since the expected value of `objective` should be `squared_error`. Based on the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">documentation</a>, The criterion `mse` was deprecated and change to `squared_error` while the criterion `mae` was deprecated and change to `absolute_error`. However, the `objective_name_map` in the SHAP library does not reflect the latest changes. Since the criterion `squared_error` is used, the `objective` is manually assigned with the value `squared_error`, just like how `mse` should be mapped to `squared_error` based on the lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "042cba2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "internal_dtype : <class 'numpy.float64'>\n",
      "criterion      : squared_error\n",
      "objective      : None\n"
     ]
    }
   ],
   "source": [
    "print(f'internal_dtype : {cp_trf_model.estimators_[0].tree_.value.dtype.type}')\n",
    "print(f'criterion      : {cp_trf_model.criterion}')\n",
    "print(f'objective      : {objective_name_map.get(cp_trf_model.criterion, None)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcfe369",
   "metadata": {},
   "source": [
    "Below is the code that extracts all the tree weights from the Scikit-learn object. Only the first element of the `trees` is investigated.\n",
    "\n",
    "```python\n",
    "scaling = 1.0 / len(model.estimators_) # output is average of trees\n",
    "self.trees = [SingleTree(e.tree_, scaling=scaling, data=data, data_missing=data_missing) \\\n",
    "              for e in model.estimators_]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0682f2eb",
   "metadata": {},
   "source": [
    "Below are the expected values for `children_left`, `children_right`, `children_default`, `features`, `thresholds`, `values` and `node_sample_weight` field.\n",
    "\n",
    "The arrays below contains the *N* array elements, where *N* is the total number of nodes in the Scikit-learn random forest regressor. The root node's index position, *i*, is 0. \n",
    "\n",
    "Below is the description for each field value:\n",
    "\n",
    "1. children_left: The children_left\\[*i*\\] contains the index position of the left child node of the node *i*. If the node *i* is a leaf node, then the value at index position *i* is set to -1.\n",
    "\n",
    "2. children_right: The children_right\\[*i*\\] contains the index position of the right child node of the node *i*. If the node *i* is a leaf node, then the value at index position *i* is set to -1.\n",
    "\n",
    "3. features, thresholds: The split feature and split threshold for node *i*. If the node *i* is a leaf node, then the value at index position *i* is set to -2. Note that the split threshold values for nominal features are always 0.5.\n",
    "\n",
    "4. values: The prediction value for node *i*. \n",
    "\n",
    "5. node_sample_weight: The weighted number of training samples reaching node *i*.\n",
    "\n",
    "For example, features\\[children_right\\[0\\]\\] is the feature of the right child of the root node. \n",
    "\n",
    "It can be observed that the values of `children_default` is the same as `children_left` since the  Scikit-learn random forest regressor does not have default children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d60bf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children_left     : [ 1  2  3  4  5  6  7  8  9 10 -1 -1 13 -1 15 -1 17 18 -1 -1]\n",
      "children_default  : [ 1  2  3  4  5  6  7  8  9 10 -1 -1 13 -1 15 -1 17 18 -1 -1]\n",
      "children_right    : [250 231 222 133  88  57  24  23  12  11  -1  -1  14  -1  16  -1  20  19\n",
      "  -1  -1]\n",
      "features          : [3, 9, 14, 1, 8, 0, 3, 55, 4, 2]\n",
      "thresholds        : [1960.0, 241.5, 0.5, 90137.0, 1558.0, 2015.5, 1495.5, 0.5, 2415.0, 3852.5]\n",
      "values            : [[4673.3377], [2773.3733], [2492.4839], [2405.4913]]\n",
      "node_sample_weight: [8533.0, 4479.0, 4171.0, 4015.0, 2290.0, 1587.0, 1104.0, 342.0, 313.0, 77.0]\n"
     ]
    }
   ],
   "source": [
    "first_single_tree = cp_tree_ensemble.trees[0]\n",
    "\n",
    "MAX_NODES = 10\n",
    "\n",
    "print(f'children_left     : {first_single_tree.children_left[:MAX_NODES+10]}')\n",
    "print(f'children_default  : {first_single_tree.children_default[:MAX_NODES+10]}')\n",
    "print(f'children_right    : {first_single_tree.children_right[:MAX_NODES+10]}')\n",
    "print(f'features          : {first_single_tree.features[:MAX_NODES].tolist()}')\n",
    "print(f'thresholds        : {first_single_tree.thresholds[:MAX_NODES].tolist()}')\n",
    "values = np.around(first_single_tree.values[:MAX_NODES], 4)\n",
    "print(f'values            : {values[:4].tolist()}')\n",
    "print(f'node_sample_weight: {first_single_tree.node_sample_weight[:MAX_NODES].tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b165298",
   "metadata": {},
   "source": [
    "Below is the code that extracts the `children_left`, `children_right`, `features`, `thresholds`, `values`, and `node_sample_weight` from the Scikit-learn object, as implemented in the <a href=\"https://github.com/slundberg/shap/blob/master/shap/explainers/_tree.py#L1137\">SHAP's source code</a>. \n",
    "\n",
    "It can be observed that:\n",
    "1. Each element in the `values` contains the prediction value, which represents the computed mean of weighted targets at node *i*. Each prediction value's shape is (1,) since the number of model output is 1. The `values` is scaled down by the total number of base learners. \n",
    "\n",
    "2. Each element in `n_node_samples` contains the actual count of samples at node *i*. \n",
    "\n",
    "3. Similar to `n_node_samples`, each element in `weighted_n_node_samples` also contains the actual count of samples at node *i*, but weighted by the sample_weight. If the bootstrap hyperparameter is set to False during fitting, the `n_node_samples` will be equivalent to `weighted_n_node_samples` since all samples are equally weighted, assuming that the sample_weight is not explicitly given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dc75470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children_left     : [ 1  2  3  4  5  6  7  8  9 10 -1 -1 13 -1 15 -1 17 18 -1 -1]\n",
      "children_right    : [250 231 222 133  88  57  24  23  12  11  -1  -1  14  -1  16  -1  20  19\n",
      "  -1  -1]\n",
      "features          : [3, 9, 14, 1, 8, 0, 3, 55, 4, 2]\n",
      "thresholds        : [1960.0, 241.5, 0.5, 90137.0, 1558.0, 2015.5, 1495.5, 0.5, 2415.0, 3852.5]\n",
      "original values   : [[[70100.0662]], [[41600.6001]], [[37387.2582]], [[36082.3701]]]\n",
      "converted values  : [[4673.3377], [2773.3733], [2492.4839], [2405.4913]]\n",
      "bootstrap enabled : True\n",
      "node_sample       : [5385, 2826, 2629, 2536, 1453, 1010, 705, 219, 199, 50]\n",
      "node_sample_weight: [8533.0, 4479.0, 4171.0, 4015.0, 2290.0, 1587.0, 1104.0, 342.0, 313.0, 77.0]\n"
     ]
    }
   ],
   "source": [
    "decision_tree = cp_trf_model.estimators_[0].tree_\n",
    "\n",
    "print(f'children_left     : {decision_tree.children_left[:MAX_NODES+10]}')\n",
    "print(f'children_right    : {decision_tree.children_right[:MAX_NODES+10]}')\n",
    "print(f'features          : {decision_tree.feature[:MAX_NODES].tolist()}')\n",
    "print(f'thresholds        : {decision_tree.threshold[:MAX_NODES].tolist()}')\n",
    "values = np.around(decision_tree.value[:MAX_NODES], 4)\n",
    "print(f'original values   : {values[:4].tolist()}')\n",
    "\n",
    "# Reshape to (total_number_of_nodes, number_of_outputs * max_number_of_classes)\n",
    "# max_number_of_classes is always 1 for regression\n",
    "values = decision_tree.value.reshape(\n",
    "    decision_tree.value.shape[0], \n",
    "    decision_tree.value.shape[1] * decision_tree.value.shape[2])\n",
    "# Divide by total number of base learners\n",
    "scaling = 1 / len(cp_trf_model.estimators_)\n",
    "values = values * scaling\n",
    "values = np.around(values, 4)\n",
    "\n",
    "print(f'converted values  : {values[:4].tolist()}')\n",
    "\n",
    "print(f'bootstrap enabled : {cp_trf_model.bootstrap}')\n",
    "print(f'node_sample       : {decision_tree.n_node_samples[:MAX_NODES].tolist()}')\n",
    "print(f'node_sample_weight: {decision_tree.weighted_n_node_samples[:MAX_NODES].tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4c58b8",
   "metadata": {},
   "source": [
    "## Tree Weights Extraction (River)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439dd93e",
   "metadata": {},
   "source": [
    "Given the information above, the tree weights extraction for Adaptive Random Forest Regressor is as shown below:\n",
    "\n",
    "1. The `internal_dtype` is set to `np.float64` as the data type of the tree weights.\n",
    "2. The `input_dtype` is set to `np.float64` since it is not a model implemented by Sciki-learn.\n",
    "3. The `objective` is set to `squared_error` to use the mean squared error as the metric to explain model loss.\n",
    "4. The `tree_output` is set to `raw_value` since the model output directly output the target variable when `predict_one` is called.\n",
    "5. The `base_offset` is set to 0 since the regressor does not use gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e771c3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "internal_dtype : <class 'numpy.float64'>\n",
      "input_dtype    : <class 'numpy.float64'>\n",
      "objective      : squared_error\n",
      "tree_output    : raw_value\n",
      "base_offset    : 0\n"
     ]
    }
   ],
   "source": [
    "arf_rg_dict = {\n",
    "    \"internal_dtype\" : np.float64,\n",
    "    \"input_dtype\"    : np.float64,\n",
    "    \"objective\"      : 'squared_error',\n",
    "    \"tree_output\"    : 'raw_value', \n",
    "    \"base_offset\"    : 0\n",
    "}\n",
    "print(f'internal_dtype : {arf_rg_dict[\"internal_dtype\"]}')\n",
    "print(f'input_dtype    : {arf_rg_dict[\"input_dtype\"]}')\n",
    "print(f'objective      : {arf_rg_dict[\"objective\"]}')\n",
    "print(f'tree_output    : {arf_rg_dict[\"tree_output\"]}')\n",
    "print(f'base_offset    : {arf_rg_dict[\"base_offset\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a8e2e6",
   "metadata": {},
   "source": [
    "Unlike Scikit-learn implemented decision trees, Hoeffding trees stores tree weights in Python objects instead of Numpy array. Each Hoeffding tree object contains an attribute called `_root`. If the root node is a parent node, the left child object and right child object can be accessed by using the syntax `_root.children[0]` and `_root.children[1]`, respectively. Since the dictionary expects an array of values, the tree weights like `features` and `thresholds` must be fetched from each node object, one at a time.\n",
    "\n",
    "Below describes the syntax to retrieve the tree weights from each node in a Hoeffding decision tree regressor.\n",
    "\n",
    "1. children_left:  If the current `node` is a branch node (subclass of `DTBranch`), the left child object can be accessed by using the syntax `node.children[0]`. \n",
    "\n",
    "\n",
    "2. children_right: If the current `node` is a branch node (subclass of `DTBranch`), the right child object can be accessed by using the syntax `node.children[1]`. \n",
    "\n",
    "\n",
    "3. children_default: Like Scikit-learn's decision trees, Hoeffding tree does not have default children. Hence, the `children_left` is assigned to `children_default` as the substitute.\n",
    "\n",
    "\n",
    "4. features: If the current `node` is a branch node and the class type is `NumericBinaryBranch`, the **numerical** split feature can be accessed by using the syntax `node.feature` Else if the current `node` is a branch node and the class type is `NominalBinaryBranch`, the **nominal** split feature can be accessed by using the syntax `node.feature`.\n",
    "\n",
    "\n",
    "5. thresholds: If the current `node` is a branch node and the class type is `NumericBinaryBranch`, the **split threshold** can be accessed by using the syntax `node.threshold`. Else if the current `node` is a branch node and the class type is `NominalBinaryBranch`, the **split value** can be accessed by using the syntax `node.value`. The **split value** cannot be directly copied to the array since the logic is different. The conversion is discussed in *FYP2_ARF_to_Dict_Conversion.ipynb*.\n",
    "\n",
    "\n",
    "6. values: Both parent nodes and leaf nodes contain an attribute called `stats`. The `stats` attribute references a `Var` class instance that stores the statistical information to calculate the prediction value. The prediction value is retrieved by using the syntax `node.stats.mean.get()`, as documented in the <a href=\"https://github.com/online-ml/river/blob/main/river/tree/nodes/htr_nodes.py#L68\">source code</a>. However, it was later proved that the `node.stats.mean.get()` cannot be used to calculate accurate SHAP values when continously training on new samples. The details can be found at `FYP2_Car_Price_Explainer.ipynb`.\n",
    "\n",
    "\n",
    "7. node_sample_weight: Both parent nodes and leaf nodes contain an attribute called `stats`. The `stats` attribute references a `Var` class instance that stores the statistical information to calculate the prediction value. The weighted count of samples at the current node is retrieved from the stored statistical informaton using the syntax `node.stats.mean.n`, as documented in the <a href=\"https://github.com/online-ml/river/blob/main/river/stats/mean.py#L11\">source code</a>. Note that the `n` is an attribute in `river.stats.mean.Mean`, and the `mean` is an attribute in `river.stats.var.Var`. However, it was later proved that the `node.stats.mean.n` cannot be used to calculate accurate SHAP values when continously training on new samples. The details can be found at `FYP2_Car_Price_Explainer.ipynb`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e8158",
   "metadata": {},
   "source": [
    "### Validation using Tree SHAP Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1533255f",
   "metadata": {},
   "source": [
    "A SHAP tree explainer is initialized with the imported dictionary to check that the extraction function is successful. \n",
    "\n",
    "**Reminder**: The River model is not loaded here since the River library is incompatible with SHAP library. Instead, the dictionary containing the extracted tree weights is imported here in JSON format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f82eb1c",
   "metadata": {},
   "source": [
    "Load the dictionary containing the extracted tree weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1627555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputs/car_price/arf_rg.json', 'r') as f:\n",
    "    cp_arf_dict_serializable = json.load(f)\n",
    "\n",
    "cp_arf_dict = deserialize_arf(cp_arf_dict_serializable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614946d8",
   "metadata": {},
   "source": [
    "Load test data to compute SHAP value and serve as background dataset for SHAP tree loss explainer. Randomly choose 300 samples from the test set to serve as the background dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a649e4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (1094, 76)\n",
      "y_test shape: (1094,)\n"
     ]
    }
   ],
   "source": [
    "# Load data preprocessor\n",
    "with open('outputs/car_price/data_preprocessor.pkl', 'rb') as f:\n",
    "    cp_data_pp = pickle.load(f)\n",
    "\n",
    "# Load car test dataset\n",
    "cp_test = pd.read_csv(f'outputs/car_price/car_test_processed.csv')\n",
    "\n",
    "# Preprocess X test\n",
    "cp_X_test = cp_test.drop(columns=['price', 'model'], axis=1)\n",
    "cp_X_test = cp_data_pp.preprocess(cp_X_test)\n",
    "cp_y_test = cp_test['price']\n",
    "\n",
    "print(f'X_test shape: {cp_X_test.shape}')\n",
    "print(f'y_test shape: {cp_y_test.shape}')\n",
    "\n",
    "# Choose a subset of the cp_X_test\n",
    "rng = np.random.default_rng(2022)\n",
    "idx_arr = rng.choice(range(len(cp_X_test)), 300)\n",
    "cp_X_test_subsample = cp_X_test.iloc[idx_arr, :].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485b2182",
   "metadata": {},
   "source": [
    "Initialize the SHAP tree explainer using the deserialized dictionary and calculate the SHAP value. The details of the explainer is discussed in *FYP2_Car_Price_Explainer.ipynb*. In this case, the SHAP tree explainer is using `tree_path_dependent` approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47a32db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used in initialization: <class 'dict'>\n",
      "Average time taken to calculate SHAP value: 2.182078218460083 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compute SHAP value\n",
    "start = time.time()\n",
    "num_iter = 10\n",
    "\n",
    "print(f'Model used in initialization: {type(cp_arf_dict)}')\n",
    "for _ in range(num_iter):\n",
    "    cp_tree_explainer = shap.TreeExplainer(\n",
    "        model = copy.deepcopy(cp_arf_dict), \n",
    "        feature_perturbation = 'tree_path_dependent'\n",
    "    )\n",
    "    _ = cp_tree_explainer.shap_values(cp_X_test)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Average time taken to calculate SHAP value: {(end - start)/num_iter} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c99b41d",
   "metadata": {},
   "source": [
    "The SHAP tree explainer is also initialized using Scikit-learn random forest regressor and SHAP value is calculated. This is to check that the performance of SHAP value calculation is not affected when dictionary is used. The result shows that the performance of SHAP value calculation is similar to the previous performance. In this case, the SHAP tree explainer is using `tree_path_dependent` approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a06ce0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used in initialization: <class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
      "Average time taken to calculate SHAP value: 2.049235725402832 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compute SHAP value\n",
    "start = time.time()\n",
    "num_iter = 10\n",
    "\n",
    "print(f'Model used in initialization: {type(cp_trf_model)}')\n",
    "for _ in range(num_iter):\n",
    "    cp_tree_explainer_tmp = shap.TreeExplainer(\n",
    "        model = cp_trf_model, \n",
    "        feature_perturbation = 'tree_path_dependent'\n",
    "    )\n",
    "    _ = cp_tree_explainer_tmp.shap_values(cp_X_test)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Average time taken to calculate SHAP value: {(end - start)/num_iter} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c6a655",
   "metadata": {},
   "source": [
    "Initialize the SHAP tree explainer using the deserialized dictionary and calculate the SHAP value. The details of the explainer is discussed in *FYP2_Car_Price_Explainer.ipynb*. In this case, the SHAP tree explainer is using `interventional` approach. It can be observed that SHAP tree explainer using `interventional` approach is slower than the one using `tree_path_dependent` approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bdc05e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used in initialization: <class 'dict'>\n",
      "Average time taken to calculate SHAP value: 2.697571587562561 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compute SHAP value\n",
    "start = time.time()\n",
    "num_iter = 10\n",
    "\n",
    "print(f'Model used in initialization: {type(cp_arf_dict)}')\n",
    "for _ in range(num_iter):\n",
    "    cp_tree_explainer = shap.TreeExplainer(\n",
    "        model = copy.deepcopy(cp_arf_dict), \n",
    "        feature_perturbation = 'interventional', \n",
    "        data = cp_X_test_subsample\n",
    "    )\n",
    "    _ = cp_tree_explainer.shap_values(cp_X_test)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Average time taken to calculate SHAP value: {(end - start)/num_iter} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883fdbdf",
   "metadata": {},
   "source": [
    "The SHAP tree explainer is also initialized using Scikit-learn random forest regressor and SHAP value is calculated. This is to check that the performance of SHAP value calculation is not affected when dictionary is used. The result shows that the performance of SHAP value calculation is similar to the previous performance. In this case, the SHAP tree explainer is using `interventional` approach. It can be observed that SHAP tree explainer using `interventional` approach is slower than the one using `tree_path_dependent` approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e2be7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used in initialization: <class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
      "Average time taken to calculate SHAP value: 2.9476255655288695 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compute SHAP value\n",
    "start = time.time()\n",
    "num_iter = 10\n",
    "\n",
    "print(f'Model used in initialization: {type(cp_trf_model)}')\n",
    "for _ in range(num_iter):\n",
    "    cp_tree_explainer_tmp = shap.TreeExplainer(\n",
    "        model = cp_trf_model, \n",
    "        feature_perturbation = 'interventional', \n",
    "        data = cp_X_test_subsample\n",
    "    )\n",
    "    _ = cp_tree_explainer_tmp.shap_values(cp_X_test)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Average time taken to calculate SHAP value: {(end - start)/num_iter} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951b833e",
   "metadata": {},
   "source": [
    "### Validation using Tree SHAP Loss Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb19fc9",
   "metadata": {},
   "source": [
    "A SHAP tree loss explainer is initialized with the imported dictionary to check that the extraction function is successful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adab576",
   "metadata": {},
   "source": [
    "Initialize the SHAP tree loss explainer using the deserialized dictionary and calculate the SHAP loss value. The details of the explainer is discussed in *FYP2_Car_Price_Explainer.ipynb*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e8e6d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used in initialization: <class 'dict'>\n",
      "Average time taken to calculate SHAP loss value: 3.3043734312057493 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compute SHAP loss value\n",
    "start = time.time()\n",
    "num_iter = 10\n",
    "\n",
    "print(f'Model used in initialization: {type(cp_arf_dict)}')\n",
    "for _ in range(num_iter):\n",
    "    cp_tree_loss_explainer = shap.TreeExplainer(\n",
    "        model = copy.deepcopy(cp_arf_dict), \n",
    "        feature_perturbation = 'interventional', \n",
    "        data = cp_X_test_subsample, \n",
    "        model_output = 'log_loss'\n",
    "    )\n",
    "    _ = cp_tree_loss_explainer.shap_values(cp_X_test, cp_y_test)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Average time taken to calculate SHAP loss value: {(end - start)/num_iter} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7667250c",
   "metadata": {},
   "source": [
    "The SHAP tree loss explainer is also initialized using Scikit-learn random forest regressor and SHAP loss value is calculated. This is to ensure that the performance of SHAP loss value calculation is not affected when dictionary is used. The result below shows that the performance of SHAP loss value calculation is similar to previous performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eea36bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used in initialization: <class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
      "Average time taken to calculate SHAP loss value : 3.281397891044617 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compute SHAP loss value\n",
    "start = time.time()\n",
    "num_iter = 10\n",
    "\n",
    "print(f'Model used in initialization: {type(cp_trf_model)}')\n",
    "for _ in range(num_iter):\n",
    "    cp_tree_loss_explainer_tmp = shap.TreeExplainer(\n",
    "        model = cp_trf_model, \n",
    "        feature_perturbation = 'interventional', \n",
    "        data = cp_X_test_subsample,\n",
    "        model_output = 'log_loss'\n",
    "    )\n",
    "    # The objective is manually assigned with the value squared_error\n",
    "    # to solve the bug mentioned above\n",
    "    cp_tree_loss_explainer_tmp.model.objective = 'squared_error'\n",
    "    _ = cp_tree_loss_explainer_tmp.shap_values(cp_X_test, cp_y_test)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Average time taken to calculate SHAP loss value : {(end - start)/num_iter} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2fbf16",
   "metadata": {},
   "source": [
    "### Validation using Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea826a5",
   "metadata": {},
   "source": [
    "As shown in *FYP2_Car_Price_Model_Training.ipynb*, `cp_trf_model` is the TRF model that transfer its tree structure to the ARF model where its weights are extracted to `cp_arf_dict`. In that transfer learning process, it is validated that the tree structure is exactly the same. The tree structure is the same such that for every parent node *i*, the `split feature`, `split threshold`, and `child nodes` are the same for both models. The `prediction value` and `node_sample_weight` are different since it is technically impossible to directly copy the weights from TRF model to the ARF model. For the TRF model, the prediction value and `node_sample_weight` has already pre-calculated by the splitter for each newly created node when building the tree. For the ARF model, the prediction value and `node_sample_weight` are not directly stored. Instead, a Var class instance stores the statistical information to calculate the prediction value and the `node_sample_weight`is retrieved from the statistical information.\n",
    "\n",
    "\n",
    "Given the high similarity in tree structures, the `cp_trf_model`'s weights are extracted to a dictionary named `cp_trf_dict` to compare with `cp_arf_dict`. The `cp_trf_dict` is used to initialize a SHAP tree explainer to verify its correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fc5cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_trf_dict = {\n",
    "    'internal_dtype' : cp_trf_model.estimators_[0].tree_.value.dtype.type,\n",
    "    'input_dtype'    : np.float32,\n",
    "    'objective'      : 'squared_error',\n",
    "    'tree_output'    : 'raw_value',\n",
    "    'base_offset'    : 0,\n",
    "    'trees'          : []\n",
    "}\n",
    "\n",
    "for dt in cp_trf_model.estimators_:\n",
    "    dt = dt.tree_\n",
    "    \n",
    "    singletree = {\n",
    "        'children_left'     : dt.children_left,\n",
    "        'children_right'    : dt.children_right,\n",
    "        'children_default'  : dt.children_left,\n",
    "        'features'          : dt.feature,\n",
    "        'thresholds'        : dt.threshold,\n",
    "        'node_sample_weight': dt.weighted_n_node_samples\n",
    "    }\n",
    "    \n",
    "    values = dt.value.reshape(\n",
    "        dt.value.shape[0], \n",
    "        dt.value.shape[1] * dt.value.shape[2]\n",
    "    )\n",
    "    # Divide by total number of base learners\n",
    "    scaling = 1 / len(cp_trf_model.estimators_)\n",
    "    values = values * scaling\n",
    "        \n",
    "    singletree['values'] = values\n",
    "    \n",
    "    cp_trf_dict['trees'].append(singletree)\n",
    "    \n",
    "# Compute SHAP loss value\n",
    "cp_tree_loss_explainer_tmp2 = shap.TreeExplainer(\n",
    "    model = copy.deepcopy(cp_trf_dict), \n",
    "    feature_perturbation = 'interventional', \n",
    "    data = cp_X_test_subsample\n",
    ")\n",
    "_ = cp_tree_loss_explainer_tmp2.shap_values(cp_X_test, cp_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc982b50",
   "metadata": {},
   "source": [
    "**Test 1**: The field values `internal_dtype`, `objective`, `tree_output`, `base_offset` for both dictionaries are the same. As mentioned earlier, for the Scikit-learn-implemented tree models, the `input_dtype` value must be set to `np.float32` to get the exact matches to predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a44f1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn random forest regressor:\n",
      "\n",
      "\tinternal_dtype: <class 'numpy.float64'>\n",
      "\tinput_dtype   : <class 'numpy.float32'>\n",
      "\tobjective     : squared_error\n",
      "\ttree_output   : raw_value\n",
      "\tbase_offset   : 0\n",
      "\n",
      "River adaptive random forest regressor:\n",
      "\n",
      "\tinternal_dtype: <class 'numpy.float64'>\n",
      "\tinput_dtype   : <class 'numpy.float64'>\n",
      "\tobjective     : squared_error\n",
      "\ttree_output   : raw_value\n",
      "\tbase_offset   : 0\n"
     ]
    }
   ],
   "source": [
    "print('Scikit-learn random forest regressor:\\n')\n",
    "print(f'\\tinternal_dtype: {cp_trf_dict[\"internal_dtype\"]}')\n",
    "print(f'\\tinput_dtype   : {cp_trf_dict[\"input_dtype\"]}')\n",
    "print(f'\\tobjective     : {cp_trf_dict[\"objective\"]}')\n",
    "print(f'\\ttree_output   : {cp_trf_dict[\"tree_output\"]}')\n",
    "print(f'\\tbase_offset   : {cp_trf_dict[\"base_offset\"]}')\n",
    "\n",
    "print('\\nRiver adaptive random forest regressor:\\n')\n",
    "print(f'\\tinternal_dtype: {cp_arf_dict[\"internal_dtype\"]}')\n",
    "print(f'\\tinput_dtype   : {cp_arf_dict[\"input_dtype\"]}')\n",
    "print(f'\\tobjective     : {cp_arf_dict[\"objective\"]}')\n",
    "print(f'\\ttree_output   : {cp_arf_dict[\"tree_output\"]}')\n",
    "print(f'\\tbase_offset   : {cp_arf_dict[\"base_offset\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ac7a3",
   "metadata": {},
   "source": [
    "**Test 2**: The position of all the nodes must be the same. It must be ensured that for every *i*th node, the split feature and split threshold must be the same between the two dictionaries. Note that the node's array index position for *i*th node is often not the same between the two dictionaries. Hence, the node index for decision tree and Hoeffding tree must be tracked separately to access the respective arrays of tree weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "088820e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_learner_no</th>\n",
       "      <th>dt_node_idx</th>\n",
       "      <th>ht_node_idx</th>\n",
       "      <th>same_feature?</th>\n",
       "      <th>same_threshold?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>231</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>251</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>222</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>232</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  base_learner_no dt_node_idx ht_node_idx same_feature? same_threshold?\n",
       "0               1           0           0          True            True\n",
       "1               1           1           1          True            True\n",
       "2               1         250           2          True            True\n",
       "3               1           2           3          True            True\n",
       "4               1         231           4          True            True\n",
       "5               1         251           5          True            True\n",
       "6               1         400           6          True            True\n",
       "7               1           3           7          True            True\n",
       "8               1         222           8          True            True\n",
       "9               1         232           9          True            True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([], columns=['base_learner_no', 'dt_node_idx', 'ht_node_idx', \n",
    "                               'same_feature?', 'same_threshold?'])\n",
    "\n",
    "node_id = -1\n",
    "\n",
    "for base_learner_no in range(len(cp_trf_model.estimators_)):\n",
    "    queue = []\n",
    "    # Separately track the node index position for both dictionaries\n",
    "    queue.append((0, 0))\n",
    "\n",
    "    while len(queue) > 0:\n",
    "        dt_node_idx, ht_node_idx = queue.pop(0)\n",
    "        dt_node_idx = int(dt_node_idx)\n",
    "        ht_node_idx = int(ht_node_idx)\n",
    "\n",
    "        # Retrieve the features from the dictionaries\n",
    "        ht_node_feature = cp_arf_dict[\"trees\"][base_learner_no][\"features\"][ht_node_idx]\n",
    "        dt_node_feature = cp_trf_dict[\"trees\"][base_learner_no][\"features\"][dt_node_idx]\n",
    "\n",
    "        # Retrieve the threshold from the dictionaries\n",
    "        ht_node_threshold = cp_arf_dict[\"trees\"][base_learner_no][\"thresholds\"][ht_node_idx]\n",
    "        dt_node_threshold = cp_trf_dict[\"trees\"][base_learner_no][\"thresholds\"][dt_node_idx]\n",
    "\n",
    "        # Add records to dataframe for debugging\n",
    "        node_id += 1\n",
    "        df.loc[node_id, :] = [base_learner_no+1, dt_node_idx, ht_node_idx, \n",
    "                              ht_node_feature == dt_node_feature, \n",
    "                              ht_node_threshold == dt_node_threshold]\n",
    "\n",
    "        # Retrieve the left child index position from the dictionaries\n",
    "        ht_left_ch_idx = cp_arf_dict[\"trees\"][base_learner_no][\"children_left\"][ht_node_idx]\n",
    "        dt_left_ch_idx = cp_trf_dict[\"trees\"][base_learner_no][\"children_left\"][dt_node_idx]\n",
    "\n",
    "        # Retrieve the right child index position from the dictionaries\n",
    "        ht_right_ch_idx = cp_arf_dict[\"trees\"][base_learner_no][\"children_right\"][ht_node_idx]\n",
    "        dt_right_ch_idx = cp_trf_dict[\"trees\"][base_learner_no][\"children_right\"][dt_node_idx]\n",
    "\n",
    "        # Only enqueue if the child node is a branch node\n",
    "        if dt_left_ch_idx != -1:\n",
    "            queue.append((dt_left_ch_idx, ht_left_ch_idx))\n",
    "        if dt_right_ch_idx != -1:\n",
    "            queue.append((dt_right_ch_idx, ht_right_ch_idx))\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bca6c0",
   "metadata": {},
   "source": [
    "The number of problematic nodes should be zero as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb972356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of problematic nodes: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_learner_no</th>\n",
       "      <th>dt_node_idx</th>\n",
       "      <th>ht_node_idx</th>\n",
       "      <th>same_feature?</th>\n",
       "      <th>same_threshold?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [base_learner_no, dt_node_idx, ht_node_idx, same_feature?, same_threshold?]\n",
       "Index: []"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problematic_nodes = df[(~df['same_feature?']) | (~df['same_threshold?'])]\n",
    "print(f'Number of problematic nodes: {problematic_nodes.shape[0]}')\n",
    "problematic_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955be514",
   "metadata": {},
   "source": [
    "**Test 3**: If test 2 is successful, check the average weights difference between Scikit-learn RF regressor and River ARF regressor. The average weights difference for `features` and `thresholds` must be zero since the test 2 has passed. \n",
    "\n",
    "Based on the result below:\n",
    "1. The first set of base learners have the highest average `values` difference of around 140.0. The difference is negligible and acceptable since the difference only accounts for 2.7% (140/5192) of the average `values` in the first decision tree.\n",
    "2. The third set of base learners have the highest average `node_sample_weight` difference of around 7.5. The difference is negligible and acceptable since the difference only accounts for 2.3% (7.5/330) of the average `node_sample_weight` in the third decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25124f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average weights difference between Scikit-learn RF regressor and River ARF regressor:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>thresholds</th>\n",
       "      <th>values</th>\n",
       "      <th>node_sample_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138.454519</td>\n",
       "      <td>5.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87.346089</td>\n",
       "      <td>5.990566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120.470146</td>\n",
       "      <td>7.460674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86.57759</td>\n",
       "      <td>4.44582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>78.85451</td>\n",
       "      <td>3.808442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76.634644</td>\n",
       "      <td>6.287037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101.776429</td>\n",
       "      <td>6.02349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85.88998</td>\n",
       "      <td>5.21118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84.723226</td>\n",
       "      <td>4.815934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80.926406</td>\n",
       "      <td>5.27673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87.458815</td>\n",
       "      <td>4.643357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90.113701</td>\n",
       "      <td>5.30721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70.746923</td>\n",
       "      <td>6.79096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79.262196</td>\n",
       "      <td>5.262346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>125.908965</td>\n",
       "      <td>5.678832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   features thresholds      values node_sample_weight\n",
       "0         0          0  138.454519           5.473684\n",
       "1         0          0   87.346089           5.990566\n",
       "2         0          0  120.470146           7.460674\n",
       "3         0          0    86.57759            4.44582\n",
       "4         0          0    78.85451           3.808442\n",
       "5         0          0   76.634644           6.287037\n",
       "6         0          0  101.776429            6.02349\n",
       "7         0          0    85.88998            5.21118\n",
       "8         0          0   84.723226           4.815934\n",
       "9         0          0   80.926406            5.27673\n",
       "10        0          0   87.458815           4.643357\n",
       "11        0          0   90.113701            5.30721\n",
       "12        0          0   70.746923            6.79096\n",
       "13        0          0   79.262196           5.262346\n",
       "14        0          0  125.908965           5.678832"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of values at the first DT node        : 5192.022179869883\n",
      "Average of node_sample_weight at third DT node: 330.10601719197706\n"
     ]
    }
   ],
   "source": [
    "weight_types = ['features', 'thresholds', 'values', 'node_sample_weight']\n",
    "\n",
    "df = pd.DataFrame([], columns=weight_types)\n",
    "\n",
    "for weight_type in weight_types:\n",
    "    for idx in range(len(cp_trf_model.estimators_)):\n",
    "        dt_weights = np.sort(cp_arf_dict[\"trees\"][idx][weight_type].flatten())\n",
    "        ht_weights = np.sort(cp_trf_dict[\"trees\"][idx][weight_type].flatten())\n",
    "        diffs = ht_weights - dt_weights\n",
    "        diffs = diffs[diffs != 0]\n",
    "        diffs_mean = np.abs(diffs).mean() if diffs.shape[0] > 0 else 0\n",
    "        df.loc[idx, f'{weight_type}'] = diffs_mean\n",
    "\n",
    "print('Average weights difference between Scikit-learn RF regressor and River ARF regressor:')\n",
    "display(df)\n",
    "\n",
    "print(f'Average of values at the first DT node        : '+\n",
    "      f'{cp_arf_dict[\"trees\"][0][\"values\"].flatten().mean()}')\n",
    "\n",
    "print(f'Average of node_sample_weight at third DT node: '+\\\n",
    "      f'{cp_arf_dict[\"trees\"][2][\"node_sample_weight\"].flatten().mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b250aa",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfea135",
   "metadata": {},
   "source": [
    "For initializing Tree SHAP explainer, the function internally extract the tree weights from an API-specific tree models before \n",
    "storing the information in the `TreeEnsemble` class instance. The function directly supports the weights extraction of tree models from commonly used API like Scikit-learn and XGBoost. However, the function does not directly supports the weight extraction of `AdaptiveRandomForestClassifier` from River API. Instead, the function accepts a Python dictionary that stores the tree weights which are `children_left`, `children_right`, `features`, `thresholds`, `node_sample_weight`, and `values`. Thus, the tree weights must be manually extracted into a dictionary.\n",
    "\n",
    "To ensure the adaptive random forest classifier's tree weights are passed in the correct format, the author initializes a Tree SHAP explainer object with the random forest classifier implemented by Scikit-learn. The tree weights are then retrieved from `TreeEnsemble` class instance to understand and prove the correctness of the values that are passed into the explainer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da9f771",
   "metadata": {},
   "source": [
    "## Tree Weights Extraction (Scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adde1fb",
   "metadata": {},
   "source": [
    "Load  the fitted Scikit-learn random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb890820",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputs/lead_scoring/trf_cf.pkl', 'rb') as f:\n",
    "    ls_trf_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e3c1a0",
   "metadata": {},
   "source": [
    "Initialize the SHAP tree explainer with the random forest classifier implemented by Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "471a1ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_tree_exp = shap.TreeExplainer(model = ls_trf_model, \n",
    "                                 feature_perturbation = 'tree_path_dependent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddacd2c",
   "metadata": {},
   "source": [
    "Based on the <a href=\"https://github.com/slundberg/shap/blob/master/shap/explainers/_tree.py#L613\">SHAP's source code</a>, the expected dictionary structure is as shown below:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'internal_dtype': <?>,\n",
    "    'input_dtype'   : <?>,\n",
    "    'objective'     : <?>,\n",
    "    'tree_output'   : <?>,\n",
    "    'base_offset'   : <?>,\n",
    "    'trees'         : <?>\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9ef0b0",
   "metadata": {},
   "source": [
    "Below are the expected values for `internal_dtype`, `input_dtype`, `objective`, `tree_output`, and `base_offset` field. \n",
    "\n",
    "1. The `internal_dtype` defines the data type of the tree weights.\n",
    "\n",
    "2. The default valule of `input_dtype` is `np.float64`. If the model is implemented by Scikit-learn, the `input_dtype` value is changed to `np.float32` to get the exact matches to predictions, as documented in the <a href=\"https://github.com/slundberg/shap/blob/master/shap/explainers/_tree.py#L576\">source code</a>.\n",
    "\n",
    "3. The `objective` value defines which metric to explain when explaining the loss of the model. If the tree model is a classifier, then the explainer will explain binary crossentropy loss. Else if the tree model is a regressor, then the explainer will explain mean squared error loss. \n",
    "\n",
    "4. For Scikit-learn classifier, the tree output is set to `probability` since the classifier can output the prediction probabilities by calling `predict_proba`. \n",
    "\n",
    "5. The default value of `base offset` is 0 unless the classifier applies gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa6e2c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class name that stores tree weights: <class 'shap.explainers._tree.TreeEnsemble'>\n",
      "internal_dtype : <class 'numpy.float64'>\n",
      "input_dtype    : <class 'numpy.float32'>\n",
      "objective      : binary_crossentropy\n",
      "tree_output    : probability\n",
      "base_offset    : [0. 0.]\n"
     ]
    }
   ],
   "source": [
    "ls_tree_ensemble = ls_tree_exp.model\n",
    "\n",
    "print(f'Class name that stores tree weights: {type(ls_tree_ensemble)}')\n",
    "print(f'internal_dtype : {ls_tree_ensemble.internal_dtype}')\n",
    "print(f'input_dtype    : {ls_tree_ensemble.input_dtype}')\n",
    "print(f'objective      : {ls_tree_ensemble.objective}')\n",
    "print(f'tree_output    : {ls_tree_ensemble.tree_output}')\n",
    "print(f'base_offset    : {ls_tree_ensemble.base_offset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b25253",
   "metadata": {},
   "source": [
    "Below is the code that extracts the `internal_dtype`, and `objective` from the Scikit-learn object, as implemented in the <a href=\"https://github.com/slundberg/shap/blob/master/shap/explainers/_tree.py#L684\">SHAP's source code</a>. The remaining field values are fixed and pre-determined based on the API of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ec5c2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "internal_dtype : <class 'numpy.float64'>\n",
      "objective      : binary_crossentropy\n"
     ]
    }
   ],
   "source": [
    "print(f'internal_dtype : {ls_trf_model.estimators_[0].tree_.value.dtype.type}')\n",
    "print(f'objective      : {objective_name_map.get(ls_trf_model.criterion, None)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb5f9f",
   "metadata": {},
   "source": [
    "Below is the code that extracts all the tree weights from the Scikit-learn object. Only the first element of the `trees` is investigated.\n",
    "\n",
    "```python\n",
    "scaling = 1.0 / len(model.estimators_) # output is average of trees\n",
    "self.trees = [SingleTree(e.tree_, normalize=True, scaling=scaling, \n",
    "                         data=data, data_missing=data_missing) \\\n",
    "              for e in model.estimators_]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a440f",
   "metadata": {},
   "source": [
    "Below are the expected values for `children_left`, `children_right`, `children_default`, `features`, `thresholds`, `values` and `node_sample_weight` field.\n",
    "\n",
    "The arrays below contains the *N* array elements, where *N* is the total number of nodes in the Scikit-learn random forest classifier. The root node's index position, *i*, is 0. \n",
    "\n",
    "Below is the description for each field value:\n",
    "\n",
    "1. children_left: The children_left\\[*i*\\] contains the index position of the left child node of the node *i*. If the node *i* is a leaf node, then the value at index position *i* is set to -1.\n",
    "\n",
    "2. children_right: The children_right\\[*i*\\] contains the index position of the right child node of the node *i*. If the node *i* is a leaf node, then the value at index position *i* is set to -1.\n",
    "\n",
    "3. features, thresholds: The split feature and split threshold for node *i*. If the node *i* is a leaf node, then the value at index position *i* is set to -2. Note that the split threshold values for nominal features are always 0.5.\n",
    "\n",
    "4. values: The prediction value for node *i*. \n",
    "\n",
    "5. node_sample_weight: The weighted number of training samples reaching node *i*.\n",
    "\n",
    "For example, features\\[children_right\\[0\\]\\] is the feature of the right child of the root node. \n",
    "\n",
    "It can be observed that the values of `children_default` is the same as `children_left` since the  Scikit-learn random forest classifier does not have default children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cb04d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children_left     : [ 1  2  3  4  5 -1 -1 -1  9 -1 -1 12 13 14 -1 -1 -1 18 19 -1]\n",
      "children_default  : [ 1  2  3  4  5 -1 -1 -1  9 -1 -1 12 13 14 -1 -1 -1 18 19 -1]\n",
      "children_right    : [24 11  8  7  6 -1 -1 -1 10 -1 -1 17 16 15 -1 -1 -1 21 20 -1]\n",
      "features          : [1, 0, 7, 8, 6, -2, -2, -2, 3, -2]\n",
      "thresholds        : [563.5, 0.5, 0.5, 0.5, 0.5, -2.0, -2.0, -2.0, 0.5, -2.0]\n",
      "values            : [[0.0255, 0.0245], [0.0328, 0.0172], [0.0185, 0.0315], [0.0214, 0.0286]]\n",
      "node_sample_weight: [5219.0, 3237.0, 1076.0, 911.0, 906.0, 44.0, 862.0, 5.0, 165.0, 157.0]\n"
     ]
    }
   ],
   "source": [
    "first_single_tree = ls_tree_ensemble.trees[0]\n",
    "\n",
    "MAX_NODES = 10\n",
    "\n",
    "print(f'children_left     : {first_single_tree.children_left[:MAX_NODES+10]}')\n",
    "print(f'children_default  : {first_single_tree.children_default[:MAX_NODES+10]}')\n",
    "print(f'children_right    : {first_single_tree.children_right[:MAX_NODES+10]}')\n",
    "print(f'features          : {first_single_tree.features[:MAX_NODES].tolist()}')\n",
    "print(f'thresholds        : {first_single_tree.thresholds[:MAX_NODES].tolist()}')\n",
    "values = np.around(first_single_tree.values[:MAX_NODES], 4)\n",
    "print(f'values            : {values[:4].tolist()}')\n",
    "print(f'node_sample_weight: {first_single_tree.node_sample_weight[:MAX_NODES].tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e08e9e",
   "metadata": {},
   "source": [
    "Below is the code that extracts the `children_left`, `children_right`, `features`, `thresholds`, `values`, and `node_sample_weight` from the Scikit-learn object, as implemented in the <a href=\"https://github.com/slundberg/shap/blob/master/shap/explainers/_tree.py#L1137\">SHAP's source code</a>. \n",
    "\n",
    "It can be observed that:\n",
    "1. Each element in the `values` contains the prediction value, which represents the computed weights for each class at node *i*. Each prediction value's shape is (2,) since the number of model output is 1 and the number of class labels is 2. The `values` is normalized and scaled down by the total number of base learners.\n",
    "\n",
    "2. Each element in `n_node_samples` contains the actual count of samples at node *i*. \n",
    "\n",
    "3. Similar to `n_node_samples`, each element in `weighted_n_node_samples` also contains the actual count of samples at node *i*, but weighted by the sample_weight. If the bootstrap hyperparameter is set to False during fitting, the `n_node_samples` will be equivalent to `weighted_n_node_samples` since all samples are equally weighted, assuming that the sample_weight is not explicitly given.\n",
    "\n",
    "4. Each element in `weighted_n_node_samples` is equivalent to the sum of the corresponding element in `values`, regardless of the bootstrap parameter. Taking the first few elements as an example, 5219 = 2663 + 2556, 3237 = 2123 + 1114, 1076 = 399 + 677 and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e427c65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children_left     : [ 1  2  3  4  5 -1 -1 -1  9 -1 -1 12 13 14 -1 -1 -1 18 19 -1]\n",
      "children_right    : [24 11  8  7  6 -1 -1 -1 10 -1 -1 17 16 15 -1 -1 -1 21 20 -1]\n",
      "features          : [1, 0, 7, 8, 6, -2, -2, -2, 3, -2]\n",
      "thresholds        : [563.5, 0.5, 0.5, 0.5, 0.5, -2.0, -2.0, -2.0, 0.5, -2.0]\n",
      "original values   : [[[2663.0, 2556.0]], [[2123.0, 1114.0]], [[399.0, 677.0]], [[390.0, 521.0]]]\n",
      "converted values  : [[0.0255, 0.0245], [0.0328, 0.0172], [0.0185, 0.0315], [0.0214, 0.0286]]\n",
      "bootstrap enabled : True\n",
      "node_sample       : [3303, 2063, 671, 567, 565, 25, 540, 2, 104, 97]\n",
      "node_sample_weight: [5219.0, 3237.0, 1076.0, 911.0, 906.0, 44.0, 862.0, 5.0, 165.0, 157.0]\n"
     ]
    }
   ],
   "source": [
    "decision_tree = ls_trf_model.estimators_[0].tree_\n",
    "\n",
    "print(f'children_left     : {decision_tree.children_left[:MAX_NODES+10]}')\n",
    "print(f'children_right    : {decision_tree.children_right[:MAX_NODES+10]}')\n",
    "print(f'features          : {decision_tree.feature[:MAX_NODES].tolist()}')\n",
    "print(f'thresholds        : {decision_tree.threshold[:MAX_NODES].tolist()}')\n",
    "values = np.around(decision_tree.value[:MAX_NODES], 4)\n",
    "print(f'original values   : {values[:4].tolist()}')\n",
    "\n",
    "# Reshape to (total_number_of_nodes, number_of_outputs * max_number_of_classes)\n",
    "values = decision_tree.value.reshape(\n",
    "    decision_tree.value.shape[0], \n",
    "    decision_tree.value.shape[1] * decision_tree.value.shape[2])\n",
    "# Normalize the prediction value using sum for each node\n",
    "values = (values.T / values.sum(1)).T\n",
    "# Divide by total number of base learners\n",
    "scaling = 1 / len(ls_trf_model.estimators_)\n",
    "values = values * scaling\n",
    "values = np.around(values, 4)\n",
    "\n",
    "print(f'converted values  : {values[:4].tolist()}')\n",
    "\n",
    "print(f'bootstrap enabled : {cp_trf_model.bootstrap}')\n",
    "print(f'node_sample       : {decision_tree.n_node_samples[:MAX_NODES].tolist()}')\n",
    "print(f'node_sample_weight: {decision_tree.weighted_n_node_samples[:MAX_NODES].tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150af686",
   "metadata": {},
   "source": [
    "## Tree Weights Extraction (River)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faecfc5",
   "metadata": {},
   "source": [
    "Given the information above, the tree weights extraction for Adaptive Random Forest Classifier is as shown below:\n",
    "\n",
    "1. The `internal_dtype` is set to `np.float64` as the data type of the tree weights.\n",
    "2. The `input_dtype` is set to `np.float64` since it is not a model implemented by Sciki-learn.\n",
    "3. The `objective` is set to `binary_crossentropy` to use the binary crossentropy as the metric to explain model loss.\n",
    "4. The `tree_output` is set to `probability` since the model output output the prediction probabilities when `predict_proba_one` is called.\n",
    "5. The `base_offset` is set to 0 since the classifier does not use gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5889983a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "internal_dtype : <class 'numpy.float64'>\n",
      "input_dtype    : <class 'numpy.float64'>\n",
      "objective      : binary_crossentropy\n",
      "tree_output    : probability\n",
      "base_offset    : 0\n"
     ]
    }
   ],
   "source": [
    "arf_rg_dict = {\n",
    "    \"internal_dtype\" : np.float64,\n",
    "    \"input_dtype\"    : np.float64,\n",
    "    \"objective\"      : 'binary_crossentropy',\n",
    "    \"tree_output\"    : 'probability', \n",
    "    \"base_offset\"    : 0\n",
    "}\n",
    "print(f'internal_dtype : {arf_rg_dict[\"internal_dtype\"]}')\n",
    "print(f'input_dtype    : {arf_rg_dict[\"input_dtype\"]}')\n",
    "print(f'objective      : {arf_rg_dict[\"objective\"]}')\n",
    "print(f'tree_output    : {arf_rg_dict[\"tree_output\"]}')\n",
    "print(f'base_offset    : {arf_rg_dict[\"base_offset\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85ad7f0",
   "metadata": {},
   "source": [
    "Unlike Scikit-learn implemented decision trees, Hoeffding trees stores tree weights in Python objects instead of Numpy array. Each Hoeffding tree object contains an attribute called `_root`. If the root node is a parent node, the left child object and right child object can be accessed by using the syntax `_root.children[0]` and `_root.children[1]`. Since the dictionary expects an array of values, the tree weights like `features` and `thresholds` must be fetched from each node object, one at a time.\n",
    "\n",
    "Below describes the syntax to retrieve the tree weights from each node in a Hoeffding decision tree classifier.\n",
    "\n",
    "For `children_left`, `children_right`, `children_default`, `features` and `thresholds`, please refer to the same subsection in the Regression section since the syntax are the same. \n",
    "\n",
    "1. values: Both parent nodes and leaf nodes contain an attribute called `stats`. The `stats` attribute references a Python dictionary that stores the observed class weights. The prediction value is retrieved by using the syntax `node.stats`. \n",
    "    - The prediction value is calculcated differently for random forest classifier and adaptive random forest classifier. Regardless, both models use the prediction value to calculate the prediction probabilities by normalizing over the sum of the weights. \n",
    "    - Random forest classifier: When building the tree, after a new node is created, the prediction value is obtained from the node splitter as shown in the <a href=\"https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_tree.pyx#L496\">source code</a>. In the case of binary classification, the prediction value contains the weighted sample counts of the left split and right split.\n",
    "    - Adaptive random forest classifier: The class weight is updated each time a new sample reaches at the current leaf node. The weight value that updates the `stats` is not constant since it is drawn from a Poisson distribution for every incremental training, as shown in the <a href=\"https://github.com/online-ml/river/blob/main/river/ensemble/adaptive_random_forest.py#L73\">source code</a>.\n",
    "    - However, it was later proved that the `node.stats` cannot be used to calculate accurate SHAP values when continously training on new samples. The details can be found at `FYP2_Lead_Scoring_Explainer.ipynb`.\n",
    "\n",
    "\n",
    "2. node_sample_weight: Both parent nodes and leaf nodes contain an attribute called `stats`. The `stats` attribute references a Python dictionary that stores the observed class weights. Like Scikit-learn's decision tree, the weighted count of samples at the current node is calculated as the sum of the weights in `stats`. However, it was later proved that the sum of the weights in `stats` cannot be used to calculate accurate SHAP values when continously training on new samples. The details can be found at `FYP2_Lead_Scoring_Explainer.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeb8f23",
   "metadata": {},
   "source": [
    "### Validation using Tree SHAP Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d65d4a0",
   "metadata": {},
   "source": [
    "A SHAP tree explainer is initialized with the imported dictionary to check that the extraction function is successful. \n",
    "\n",
    "**Reminder**: The River model is not loaded here since the River library is incompatible with SHAP library. Instead, the dictionary containing the extracted tree weights is imported here in JSON format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834cb94e",
   "metadata": {},
   "source": [
    "Load the dictionary containing the extracted tree weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82a8699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputs/lead_scoring/arf_cf.json', 'r') as f:\n",
    "    ls_arf_dict_serializable = json.load(f)\n",
    "\n",
    "ls_arf_dict = deserialize_arf(ls_arf_dict_serializable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1a2d54",
   "metadata": {},
   "source": [
    "Load test data to compute SHAP value and serve as background dataset for SHAP tree loss explainer. Randomly choose 300 samples from the test set to serve as the background dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2181f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (1305, 9)\n",
      "y_test shape: (1305,)\n"
     ]
    }
   ],
   "source": [
    "# Load data preprocessor\n",
    "with open('outputs/lead_scoring/data_preprocessor.pkl', 'rb') as f:\n",
    "    ls_data_pp = pickle.load(f)\n",
    "\n",
    "# Load car test dataset\n",
    "ls_test = pd.read_csv(f'outputs/lead_scoring/test_set.csv')\n",
    "\n",
    "# Preprocess X test\n",
    "ls_X_test = ls_test.drop(columns=['converted', 'dont_call'], axis=1)\n",
    "ls_X_test = ls_data_pp.preprocess(ls_X_test)\n",
    "ls_y_test = ls_test['converted']\n",
    "\n",
    "print(f'X_test shape: {ls_X_test.shape}')\n",
    "print(f'y_test shape: {ls_y_test.shape}')\n",
    "\n",
    "# Choose a subset of the ls_X_test\n",
    "rng = np.random.default_rng(2022)\n",
    "idx_arr = rng.choice(range(len(ls_X_test)), 300)\n",
    "ls_X_test_subsample = ls_X_test.iloc[idx_arr, :].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b92796",
   "metadata": {},
   "source": [
    "Initialize the SHAP tree explainer using the deserialized dictionary and calculate the SHAP value. The details of the explainer is discussed in *FYP2_Lead_Scoring_Explainer.ipynb*. In this case, the SHAP tree explainer is using `tree_path_dependent` approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba14454e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used in initialization: <class 'dict'>\n",
      "Average time taken to calculate SHAP value: 0.1352790117263794 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compute SHAP value\n",
    "start = time.time()\n",
    "num_iter = 10\n",
    "\n",
    "print(f'Model used in initialization: {type(ls_arf_dict)}')\n",
    "for _ in range(num_iter):\n",
    "    ls_tree_explainer = shap.TreeExplainer(\n",
    "        model = copy.deepcopy(ls_arf_dict), \n",
    "        feature_perturbation = 'tree_path_dependent'\n",
    "    )\n",
    "    _ = ls_tree_explainer.shap_values(ls_X_test)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Average time taken to calculate SHAP value: {(end - start)/num_iter} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1582308f",
   "metadata": {},
   "source": [
    "The SHAP tree explainer is also initialized using Scikit-learn random forest classifier and SHAP value is calculated. This is to check that the performance of SHAP value calculation is not affected when dictionary is used. The result shows that the performance of SHAP value calculation is similar to the previous performance. In this case, the SHAP tree explainer is using `tree_path_dependent` approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4568190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used in initialization: <class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "Average time taken to calculate SHAP value: 0.14020864963531493 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compute SHAP value\n",
    "start = time.time()\n",
    "num_iter = 10\n",
    "\n",
    "print(f'Model used in initialization: {type(ls_trf_model)}')\n",
    "for _ in range(num_iter):\n",
    "    ls_tree_explainer_tmp = shap.TreeExplainer(\n",
    "        model = ls_trf_model, \n",
    "        feature_perturbation = 'tree_path_dependent'\n",
    "    )\n",
    "    _ = ls_tree_explainer_tmp.shap_values(ls_X_test)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Average time taken to calculate SHAP value: {(end - start)/num_iter} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de946fd2",
   "metadata": {},
   "source": [
    "Initialize the SHAP tree explainer using the deserialized dictionary and calculate the SHAP value. The details of the explainer is discussed in *FYP2_Lead_Scoring_Explainer.ipynb*. In this case, the SHAP tree explainer is using `interventional` approach. It can be observed that SHAP tree explainer using `interventional` approach is slower than the one using `tree_path_dependent` approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c1e5673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used in initialization: <class 'dict'>\n",
      "Average time taken to calculate SHAP value: 1.5576391458511352 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compute SHAP loss value\n",
    "start = time.time()\n",
    "num_iter = 10\n",
    "\n",
    "print(f'Model used in initialization: {type(ls_arf_dict)}')\n",
    "for _ in range(num_iter):\n",
    "    ls_tree_loss_explainer = shap.TreeExplainer(\n",
    "        model = copy.deepcopy(ls_arf_dict), \n",
    "        feature_perturbation = 'interventional', \n",
    "        data = ls_X_test_subsample\n",
    "    )\n",
    "    _ = ls_tree_loss_explainer.shap_values(ls_X_test)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Average time taken to calculate SHAP value: {(end - start)/num_iter} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59d0054",
   "metadata": {},
   "source": [
    "The SHAP tree explainer is also initialized using Scikit-learn random forest classifier and SHAP value is calculated. This is to check that the performance of SHAP value calculation is not affected when dictionary is used. The result shows that the performance of SHAP value calculation is similar to the previous performance. In this case, the SHAP tree explainer is using `interventional` approach. It can be observed that SHAP tree explainer using `interventional` approach is slower than the one using `tree_path_dependent` approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b70645b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used in initialization: <class 'dict'>\n",
      "Average time taken to calculate SHAP value: 1.4959629774093628 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compute SHAP loss value\n",
    "start = time.time()\n",
    "num_iter = 10\n",
    "\n",
    "print(f'Model used in initialization: {type(ls_arf_dict)}')\n",
    "for _ in range(num_iter):\n",
    "    ls_tree_loss_explainer = shap.TreeExplainer(\n",
    "        model = copy.deepcopy(ls_arf_dict), \n",
    "        feature_perturbation = 'interventional', \n",
    "        data = ls_X_test_subsample\n",
    "    )\n",
    "    _ = ls_tree_loss_explainer.shap_values(ls_X_test)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Average time taken to calculate SHAP value: {(end - start)/num_iter} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f21b58b",
   "metadata": {},
   "source": [
    "### Validation using Tree SHAP Loss Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c4caf7",
   "metadata": {},
   "source": [
    "A SHAP tree loss explainer is initialized with the imported dictionary to check that the extraction function is successful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5fa8dd",
   "metadata": {},
   "source": [
    "Initialize the SHAP tree loss explainer using the deserialized dictionary and calculate the SHAP loss value. The details of the explainer is discussed in *FYP2_Lead_Scoring_Explainer.ipynb*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4bc2831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used in initialization: <class 'dict'>\n",
      "Average time taken to calculate SHAP loss value: 1.8151605606079102 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compute SHAP loss value\n",
    "start = time.time()\n",
    "num_iter = 10\n",
    "\n",
    "print(f'Model used in initialization: {type(ls_arf_dict)}')\n",
    "for _ in range(num_iter):\n",
    "    ls_tree_loss_explainer = shap.TreeExplainer(\n",
    "        model = copy.deepcopy(ls_arf_dict), \n",
    "        feature_perturbation = 'interventional', \n",
    "        data = ls_X_test_subsample, \n",
    "        model_output = 'log_loss'\n",
    "    )\n",
    "    _ = ls_tree_loss_explainer.shap_values(ls_X_test, ls_y_test)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Average time taken to calculate SHAP loss value: {(end - start)/num_iter} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f18dc",
   "metadata": {},
   "source": [
    "The SHAP tree loss explainer is also initialized using Scikit-learn random forest classifier and SHAP loss value is calculated. This is to ensure that the performance of SHAP loss value calculation is not affected when dictionary is used. The result below shows that the performance of SHAP loss value calculation is similar to previous performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f0dc0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used in initialization: <class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "Average time taken to calculate SHAP loss value : 1.8570066690444946 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compute SHAP loss value\n",
    "start = time.time()\n",
    "num_iter = 10\n",
    "\n",
    "print(f'Model used in initialization: {type(ls_trf_model)}')\n",
    "for _ in range(num_iter):\n",
    "    ls_tree_loss_explainer_tmp = shap.TreeExplainer(\n",
    "        model = ls_trf_model, \n",
    "        feature_perturbation = 'interventional', \n",
    "        data = ls_X_test_subsample, \n",
    "        model_output = 'log_loss'\n",
    "    )\n",
    "    _ = ls_tree_loss_explainer_tmp.shap_values(ls_X_test, ls_y_test)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Average time taken to calculate SHAP loss value : {(end - start)/num_iter} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba02b1",
   "metadata": {},
   "source": [
    "### Validation using Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4f4eca",
   "metadata": {},
   "source": [
    "As shown in *FYP2_Lead_Scoring_Model_Training.ipynb*, `cp_trf_model` is the TRF model that transfer its tree structure to the ARF model where its weights are extracted to `cp_arf_dict`. In that transfer learning process, it is validated that the tree structure is exactly the same. The tree structure is the same such that for every parent node *i*, the `split feature`, `split threshold`, `child nodes`, `prediction_value`, and `node_sample_weight` are the same for both models. The `prediction value` and `node_sample_weight` are the same since the prediction value of every node is directly copied from the TRF model to the ARF model. It is technically feasible since both models use majority class prediction as the leaf prediction mechanisms to output prediction probabilities. The `node_sample_weight` is also the same since it is calculcated as the sum of weights in `prediction_value`.\n",
    "\n",
    "Given the high similarity in tree structures, the `cp_trf_model`'s weights are extracted to a dictionary named `cp_trf_dict` to compare with `cp_arf_dict`. The `cp_trf_dict` is used to initialize a SHAP tree explainer to verify its correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2070e0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_trf_dict = {\n",
    "    'internal_dtype' : ls_trf_model.estimators_[0].tree_.value.dtype.type,\n",
    "    'input_dtype'    : np.float32,\n",
    "    'objective'      : 'binary_crossentropy',\n",
    "    'tree_output'    : 'probability',\n",
    "    'base_offset'    : 0,\n",
    "    'trees'          : []\n",
    "}\n",
    "\n",
    "for dt in ls_trf_model.estimators_:\n",
    "    dt = dt.tree_\n",
    "    \n",
    "    singletree = {\n",
    "        'children_left'     : dt.children_left,\n",
    "        'children_right'    : dt.children_right,\n",
    "        'children_default'  : dt.children_left,\n",
    "        'features'          : dt.feature,\n",
    "        'thresholds'        : dt.threshold,\n",
    "        'node_sample_weight': dt.weighted_n_node_samples\n",
    "    }\n",
    "\n",
    "    values = dt.value.reshape(\n",
    "        dt.value.shape[0], \n",
    "        dt.value.shape[1] * dt.value.shape[2]\n",
    "    )\n",
    "    # Normalize the prediction value using sum for each node\n",
    "    values = (values.T / values.sum(1)).T\n",
    "    # Divide by total number of base learners\n",
    "    scaling = 1 / len(ls_trf_model.estimators_)\n",
    "    values = values * scaling\n",
    "        \n",
    "    singletree['values'] = values\n",
    "    \n",
    "    ls_trf_dict['trees'].append(singletree)\n",
    "    \n",
    "# Compute SHAP loss value\n",
    "ls_tree_loss_explainer_tmp2 = shap.TreeExplainer(\n",
    "    model = copy.deepcopy(ls_trf_dict), \n",
    "    feature_perturbation = 'interventional', \n",
    "    data = ls_X_test_subsample\n",
    ")\n",
    "_ = ls_tree_loss_explainer_tmp2.shap_values(ls_X_test, ls_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3448e4d5",
   "metadata": {},
   "source": [
    "**Test 1**: The field values `internal_dtype`, `objective`, `tree_output`, `base_offset` for both dictionaries are the same. As mentioned earlier, for the Scikit-learn-implemented tree models, the `input_dtype` value must be set to `np.float32` to get the exact matches to predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "262fb1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn random forest classifier:\n",
      "\n",
      "\tinternal_dtype: <class 'numpy.float64'>\n",
      "\tinput_dtype   : <class 'numpy.float32'>\n",
      "\tobjective     : binary_crossentropy\n",
      "\ttree_output   : probability\n",
      "\tbase_offset   : 0\n",
      "\n",
      "River adaptive random forest classifier:\n",
      "\n",
      "\tinternal_dtype: <class 'numpy.float64'>\n",
      "\tinput_dtype   : <class 'numpy.float64'>\n",
      "\tobjective     : binary_crossentropy\n",
      "\ttree_output   : probability\n",
      "\tbase_offset   : 0\n"
     ]
    }
   ],
   "source": [
    "print('Scikit-learn random forest classifier:\\n')\n",
    "print(f'\\tinternal_dtype: {ls_trf_dict[\"internal_dtype\"]}')\n",
    "print(f'\\tinput_dtype   : {ls_trf_dict[\"input_dtype\"]}')\n",
    "print(f'\\tobjective     : {ls_trf_dict[\"objective\"]}')\n",
    "print(f'\\ttree_output   : {ls_trf_dict[\"tree_output\"]}')\n",
    "print(f'\\tbase_offset   : {ls_trf_dict[\"base_offset\"]}')\n",
    "\n",
    "print('\\nRiver adaptive random forest classifier:\\n')\n",
    "print(f'\\tinternal_dtype: {ls_arf_dict[\"internal_dtype\"]}')\n",
    "print(f'\\tinput_dtype   : {ls_arf_dict[\"input_dtype\"]}')\n",
    "print(f'\\tobjective     : {ls_arf_dict[\"objective\"]}')\n",
    "print(f'\\ttree_output   : {ls_arf_dict[\"tree_output\"]}')\n",
    "print(f'\\tbase_offset   : {ls_arf_dict[\"base_offset\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd6de81",
   "metadata": {},
   "source": [
    "**Test 2**: The position of all the nodes must be the same. It must be ensured that for every *i*th node, the split feature and split threshold must be the same between the two dictionaries. Note that the node's array index position for *i*th node is often not the same between the two dictionaries. Hence, the node index for decision tree and Hoeffding tree must be tracked separately to access the respective arrays of tree weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "374dbc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_learner_no</th>\n",
       "      <th>dt_node_idx</th>\n",
       "      <th>ht_node_idx</th>\n",
       "      <th>same_feature?</th>\n",
       "      <th>same_threshold?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  base_learner_no dt_node_idx ht_node_idx same_feature? same_threshold?\n",
       "0               1           0           0          True            True\n",
       "1               1           1           1          True            True\n",
       "2               1          24           2          True            True\n",
       "3               1           2           3          True            True\n",
       "4               1          11           4          True            True\n",
       "5               1          25           5          True            True\n",
       "6               1          32           6          True            True\n",
       "7               1           3           7          True            True\n",
       "8               1           8           8          True            True\n",
       "9               1          12           9          True            True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([], columns=['base_learner_no', 'dt_node_idx', 'ht_node_idx', \n",
    "                               'same_feature?', 'same_threshold?'])\n",
    "\n",
    "node_id = -1\n",
    "\n",
    "for base_learner_no in range(len(ls_trf_model.estimators_)):\n",
    "    queue = []\n",
    "    # Separately track the node index position for both dictionaries\n",
    "    queue.append((0, 0))\n",
    "\n",
    "    while len(queue) > 0:\n",
    "        dt_node_idx, ht_node_idx = queue.pop(0)\n",
    "        dt_node_idx = int(dt_node_idx)\n",
    "        ht_node_idx = int(ht_node_idx)\n",
    "\n",
    "        # Retrieve the features from the dictionaries\n",
    "        ht_node_feature = ls_arf_dict[\"trees\"][base_learner_no][\"features\"][ht_node_idx]\n",
    "        dt_node_feature = ls_trf_dict[\"trees\"][base_learner_no][\"features\"][dt_node_idx]\n",
    "\n",
    "        # Retrieve the threshold from the dictionaries\n",
    "        ht_node_threshold = ls_arf_dict[\"trees\"][base_learner_no][\"thresholds\"][ht_node_idx]\n",
    "        dt_node_threshold = ls_trf_dict[\"trees\"][base_learner_no][\"thresholds\"][dt_node_idx]\n",
    "\n",
    "        # Add records to dataframe for debugging\n",
    "        node_id += 1\n",
    "        df.loc[node_id, :] = [base_learner_no+1, dt_node_idx, ht_node_idx, \n",
    "                              ht_node_feature == dt_node_feature, \n",
    "                              ht_node_threshold == dt_node_threshold]\n",
    "\n",
    "        # Retrieve the left child index position from the dictionaries\n",
    "        ht_left_ch_idx = ls_arf_dict[\"trees\"][base_learner_no][\"children_left\"][ht_node_idx]\n",
    "        dt_left_ch_idx = ls_trf_dict[\"trees\"][base_learner_no][\"children_left\"][dt_node_idx]\n",
    "\n",
    "        # Retrieve the right child index position from the dictionaries\n",
    "        ht_right_ch_idx = ls_arf_dict[\"trees\"][base_learner_no][\"children_right\"][ht_node_idx]\n",
    "        dt_right_ch_idx = ls_trf_dict[\"trees\"][base_learner_no][\"children_right\"][dt_node_idx]\n",
    "\n",
    "        # Only enqueue if the child node is a branch node\n",
    "        if dt_left_ch_idx != -1:\n",
    "            queue.append((dt_left_ch_idx, ht_left_ch_idx))\n",
    "        if dt_right_ch_idx != -1:\n",
    "            queue.append((dt_right_ch_idx, ht_right_ch_idx))\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea04f55",
   "metadata": {},
   "source": [
    "The number of problematic nodes should be zero as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "72510426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of problematic nodes: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_learner_no</th>\n",
       "      <th>dt_node_idx</th>\n",
       "      <th>ht_node_idx</th>\n",
       "      <th>same_feature?</th>\n",
       "      <th>same_threshold?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [base_learner_no, dt_node_idx, ht_node_idx, same_feature?, same_threshold?]\n",
       "Index: []"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problematic_nodes = df[(~df['same_feature?']) | (~df['same_threshold?'])]\n",
    "print(f'Number of problematic nodes: {problematic_nodes.shape[0]}')\n",
    "problematic_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a56751e",
   "metadata": {},
   "source": [
    "**Test 3**: If test 2 is successful, check the average weights difference between Scikit-learn RF classifier and River ARF classifier. As shown below, the average weights difference for `features`,  `thresholds`, `values`, and `node_sample_weight` must be zero since the test 2 has passed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c4803f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average weights difference between Scikit-learn RF classifier and River ARF classifier:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>thresholds</th>\n",
       "      <th>values</th>\n",
       "      <th>node_sample_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   features thresholds values node_sample_weight\n",
       "0         0          0    0.0                  0\n",
       "1         0          0    0.0                  0\n",
       "2         0          0    0.0                  0\n",
       "3         0          0    0.0                  0\n",
       "4         0          0    0.0                  0\n",
       "5         0          0    0.0                  0\n",
       "6         0          0    0.0                  0\n",
       "7         0          0    0.0                  0\n",
       "8         0          0    0.0                  0\n",
       "9         0          0    0.0                  0\n",
       "10        0          0    0.0                  0\n",
       "11        0          0    0.0                  0\n",
       "12        0          0    0.0                  0\n",
       "13        0          0    0.0                  0\n",
       "14        0          0    0.0                  0\n",
       "15        0          0    0.0                  0\n",
       "16        0          0    0.0                  0\n",
       "17        0          0    0.0                  0\n",
       "18        0          0    0.0                  0\n",
       "19        0          0    0.0                  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weight_types = ['features', 'thresholds', 'values', 'node_sample_weight']\n",
    "\n",
    "df = pd.DataFrame([], columns=weight_types)\n",
    "\n",
    "for weight_type in weight_types:\n",
    "    for idx in range(len(ls_trf_model.estimators_)):\n",
    "        dt_weights = np.sort(ls_arf_dict[\"trees\"][idx][weight_type].flatten())\n",
    "        ht_weights = np.sort(ls_trf_dict[\"trees\"][idx][weight_type].flatten())\n",
    "        diffs = ht_weights - dt_weights\n",
    "        diffs = diffs[diffs != 0]\n",
    "        diffs_mean = np.abs(diffs).mean() if diffs.shape[0] > 0 else 0\n",
    "        df.loc[idx, f'{weight_type}'] = diffs_mean\n",
    "\n",
    "print('Average weights difference between Scikit-learn RF classifier and River ARF classifier:')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f86b86",
   "metadata": {},
   "source": [
    "The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SHAP Env)",
   "language": "python",
   "name": "arf_conda_exp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "323.021px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
